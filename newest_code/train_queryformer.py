import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
import pandas as pd
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
import pickle
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from queryformer_model import (
    QueryFormerModel, QueryTokenizer, QueryCardinalityLoss, 
    print_metrics, q_error
)
from advanced_data_processor import AdvancedDataProcessor, QueryFormerDataset

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    token_ids = torch.stack([item['token_ids'] for item in batch])
    targets = torch.stack([item['target'] for item in batch])
    
    # å¤„ç†è®¡åˆ’ç‰¹å¾
    plan_features = {
        'node_types': torch.stack([item['plan_features']['node_types'] for item in batch]),
        'costs': torch.stack([item['plan_features']['costs'] for item in batch]),
        'rows': torch.stack([item['plan_features']['rows'] for item in batch]),
        'selectivities': torch.stack([item['plan_features']['selectivities'] for item in batch])
    }
    
    return {
        'token_ids': token_ids,
        'plan_features': plan_features,
        'targets': targets
    }

def train_epoch(model, dataloader, criterion, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    num_batches = len(dataloader)
    
    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}')
    
    for batch_idx, batch in enumerate(progress_bar):
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        token_ids = batch['token_ids'].to(device)
        plan_features = {k: v.to(device) for k, v in batch['plan_features'].items()}
        targets = batch['targets'].to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(token_ids, plan_features)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(outputs, targets)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Avg Loss': f'{total_loss / (batch_idx + 1):.4f}'
        })
    
    return total_loss / num_batches

def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc='Evaluating'):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            token_ids = batch['token_ids'].to(device)
            plan_features = {k: v.to(device) for k, v in batch['plan_features'].items()}
            targets = batch['targets'].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(token_ids, plan_features)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
            all_predictions.extend(outputs.cpu().numpy().flatten())
            all_targets.extend(targets.cpu().numpy().flatten())
    
    avg_loss = total_loss / len(dataloader)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    predictions = np.array(all_predictions)
    targets = np.array(all_targets)
    
    return avg_loss, predictions, targets

def save_model(model, tokenizer, processor, save_dir, epoch, loss):
    """ä¿å­˜æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(save_dir, f'queryformer_epoch_{epoch}_loss_{loss:.4f}.pth')
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'vocab_size': tokenizer.vocab_size,
            'd_model': model.d_model,
            'max_seq_length': model.max_seq_length
        },
        'epoch': epoch,
        'loss': loss
    }, model_path)
    
    # ä¿å­˜åˆ†è¯å™¨
    tokenizer_path = os.path.join(save_dir, 'tokenizer.pkl')
    with open(tokenizer_path, 'wb') as f:
        pickle.dump(tokenizer, f)
    
    # ä¿å­˜å¤„ç†å™¨
    processor_path = os.path.join(save_dir, 'processor.pkl')
    with open(processor_path, 'wb') as f:
        pickle.dump(processor, f)
    
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    return model_path

def plot_training_history(train_losses, val_losses, save_path):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒQueryFormeræ¨¡å‹')
    parser.add_argument('--train_samples', type=int, default=30000, help='è®­ç»ƒæ ·æœ¬æ•°é‡')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--d_model', type=int, default=256, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=6, help='Transformerå±‚æ•°')
    parser.add_argument('--nhead', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutç‡')
    parser.add_argument('--max_seq_length', type=int, default=512, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--save_dir', type=str, default='newest_code/models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡é€‰æ‹©')
    parser.add_argument('--early_stopping_patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--min_delta', type=float, default=1e-4, help='æ—©åœæœ€å°æ”¹è¿›é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è®¾å¤‡é€‰æ‹©
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    print("åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨...")
    processor = AdvancedDataProcessor()
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_data, train_cardinalities = processor.load_data(
        'data/train_data.json', 
        max_samples=args.train_samples
    )
    
    if not train_data:
        print("åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥ï¼")
        return
    
    # æ„å»ºè¯æ±‡è¡¨
    print("æ„å»ºè¯æ±‡è¡¨...")
    processor.build_vocabularies(train_data)
    
    # åˆ›å»ºåˆ†è¯å™¨
    print("åˆ›å»ºåˆ†è¯å™¨...")
    tokenizer = QueryTokenizer()
    queries = [item['query'] for item in train_data]
    tokenizer.build_vocab(queries)
    
    # åˆ›å»ºæ•°æ®é›†
    print("åˆ›å»ºæ•°æ®é›†...")
    full_dataset = QueryFormerDataset(
        train_data, 
        train_cardinalities, 
        processor, 
        tokenizer,
        max_seq_length=args.max_seq_length
    )
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # æ”¹ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0  # æ”¹ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = QueryFormerModel(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        nhead=args.nhead,
        num_layers=args.num_layers,
        dropout=args.dropout,
        max_seq_length=args.max_seq_length
    ).to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = QueryCardinalityLoss(alpha=0.5)
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate,
        weight_decay=1e-5
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min', 
        factor=0.5, 
        patience=5,
        verbose=True
    )
    
    # æ—©åœæœºåˆ¶å˜é‡
    best_val_loss = float('inf')
    best_model_path = None
    patience_counter = 0
    train_losses = []
    val_losses = []
    
    print("å¼€å§‹è®­ç»ƒ...")
    print(f"æ—©åœè®¾ç½®: è€å¿ƒå€¼={args.early_stopping_patience}, æœ€å°æ”¹è¿›é˜ˆå€¼={args.min_delta}")
    
    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)
        train_losses.append(train_loss)
        
        # éªŒè¯
        val_loss, val_predictions, val_targets = evaluate(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)
        
        # æ‰“å°ç»“æœ
        print(f"\nEpoch {epoch}/{args.epochs}")
        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
        print_metrics(val_targets, val_predictions, "éªŒè¯é›† ")
        
        # æ—©åœæ£€æŸ¥
        improvement = best_val_loss - val_loss
        if improvement > args.min_delta:
            # æœ‰æ˜¾è‘—æ”¹è¿›
            best_val_loss = val_loss
            patience_counter = 0
            best_model_path = save_model(
                model, tokenizer, processor, args.save_dir, epoch, val_loss
            )
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯æŸå¤±: {val_loss:.4f} (æ”¹è¿›: {improvement:.4f})")
        else:
            # æ²¡æœ‰æ˜¾è‘—æ”¹è¿›
            patience_counter += 1
            print(f"â³ éªŒè¯æŸå¤±æœªæ”¹è¿› ({patience_counter}/{args.early_stopping_patience})")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
        if patience_counter >= args.early_stopping_patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {args.early_stopping_patience} ä¸ªepochéªŒè¯æŸå¤±æœªæ”¹è¿›")
            print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            break
        
        print("-" * 50)
    
    # è®­ç»ƒç»“æŸæ€»ç»“
    if patience_counter < args.early_stopping_patience:
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼å®Œæˆäº†æ‰€æœ‰ {args.epochs} ä¸ªepoch")
    else:
        print(f"\nğŸ›‘ è®­ç»ƒå› æ—©åœè€Œç»“æŸï¼Œå®é™…è®­ç»ƒäº† {epoch} ä¸ªepoch")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_path = os.path.join(args.save_dir, 'training_history.png')
    plot_training_history(train_losses, val_losses, plot_path)
    print(f"è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ°: {plot_path}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss,
        'best_model_path': best_model_path,
        'early_stopped': patience_counter >= args.early_stopping_patience,
        'total_epochs': epoch,
        'args': vars(args)
    }
    
    history_path = os.path.join(args.save_dir, 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹: {best_model_path}")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    return best_model_path, best_val_loss

if __name__ == '__main__':
    main() 